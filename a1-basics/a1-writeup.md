# CS336 Assignment 1 Writeup (Self-study)
## rfree2006

---
## 2.1 The Unicode Standard

### Problem (unicode1): Understanding Unicode (1 point)

**(a)** What Unicode character does `chr(0)` return? 
```
# Test the code with python
c = chr(0)
print("c:", c)
print("ord(c):", ord(c))

>>>
c: 
ord(c): 0
```
**Answer:** 
    chr(0) returns the Unicode character U+0000, the NULL control character.

    chr(0) è¿”å›žçš„æ˜¯ Unicode ç ä½ U+0000 çš„ç©ºå­—ç¬¦ï¼ˆNULL æŽ§åˆ¶å­—ç¬¦ï¼‰ã€‚

**(b)** How does this characterâ€™s string representation (`__repr__()`) differ from its printed representation?  
```
c = chr(0)

print("repr(c) =", repr(c))
print("print(c) ->", end="")
print(c)
print("<- end")

s = "A" + c + "B"
print("repr(s) =", repr(s))
print("printed:", s)
print("len(s) =", len(s))

>>>
repr(c) = '\x00'
print(c) ->
<- end
repr(s) = 'A\x00B'
printed: AB
len(s) = 3
```
**Answer:** 
    In repr it appears as the escape sequence '\x00', but when printed it produces an invisible character and looks like nothing is shown.

    åœ¨ repr ä¸­å®ƒæ˜¾ç¤ºä¸ºè½¬ä¹‰åºåˆ— '\x00'ï¼Œè€Œç›´æŽ¥æ‰“å°æ—¶æ˜¯ä¸€ä¸ªä¸å¯è§çš„æŽ§åˆ¶å­—ç¬¦ï¼Œçœ‹èµ·æ¥å¥½åƒä»€ä¹ˆéƒ½æ²¡è¾“å‡ºã€‚

**(c)** What happens when this character occurs in text?  
```
c = chr(0)

print("1.", c)
print("2.", "this is a test" + c + "string")
print("3.", repr("this is a test" + c + "string"))
print("4. len:", len("this is a test" + c + "string"))

>>>
1. 
2. this is a teststring
3. 'this is a test\x00string'
4. len: 21
```
**Answer:** 
    When inserted into a string it becomes an invisible character between the surrounding text: the printed string looks normal , but repr shows \x00 and the length increases by one.

    å½“å®ƒå‡ºçŽ°åœ¨å­—ç¬¦ä¸²ä¸­æ—¶ï¼Œä¼šä½œä¸ºä¸­é—´çš„ä¸€ä¸ªä¸å¯è§å­—ç¬¦å­˜åœ¨ï¼Œæ‰“å°å‡ºæ¥çš„æ–‡æœ¬çœ‹èµ·æ¥æ­£å¸¸ï¼Œä½† repr ä¼šæ˜¾ç¤º \x00ï¼Œè€Œä¸”å­—ç¬¦ä¸²é•¿åº¦ä¼šå¤š 1ã€‚

---

## 2.2 Unicode Encodings

### Problem (unicode2): Unicode Encodings (3 points)

**(a)** What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than
UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various
input strings.
```
# The question prompts us to "compare the encoded outputs of various input strings", so we try several different forms of input "such as Chinese, words, emojis, etc." to observe the differences.
# é—®é¢˜ä¸­æç¤ºæˆ‘ä»¬â€åŽ»æ¯”è¾ƒå„ç§è¾“å…¥å­—ç¬¦ä¸²çš„è¿™äº›ç¼–ç çš„è¾“å‡ºâ€œï¼Œç”±æ­¤æˆ‘ä»¬å¤šåŽ»å°è¯•å‡ ç§ä¸åŒå½¢å¼çš„è¾“å…¥â€œåƒä¸­æ–‡ï¼Œå•è¯ï¼Œè¡¨æƒ…ç¬¦ç­‰ç­‰â€œè§‚å¯Ÿå·®å¼‚ã€‚

texts = ["hello", "ä½ å¥½", "ðŸ™‚", "Aä½ å¥½ðŸ™‚B"]

for s in texts:
    b8  = s.encode("utf-8")
    b16 = s.encode("utf-16")
    b32 = s.encode("utf-32")
    print("text:", repr(s))
    print(" utf-8 :", b8,  "len =", len(b8))
    print(" utf-16:", b16, "len =", len(b16))
    print(" utf-32:", b32, "len =", len(b32))
    print()

>>> 
text: 'hello'
 utf-8 : b'hello' len = 5
 utf-16: b'\xff\xfeh\x00e\x00l\x00l\x00o\x00' len = 12
 utf-32: b'\xff\xfe\x00\x00h\x00\x00\x00e\x00\x00\x00l\x00\x00\x00l\x00\x00\x00o\x00\x00\x00' len = 24

text: 'ä½ å¥½'
 utf-8 : b'\xe4\xbd\xa0\xe5\xa5\xbd' len = 6
 utf-16: b'\xff\xfe`O}Y' len = 6
 utf-32: b'\xff\xfe\x00\x00`O\x00\x00}Y\x00\x00' len = 12

text: 'ðŸ™‚'
 utf-8 : b'\xf0\x9f\x99\x82' len = 4
 utf-16: b'\xff\xfe=\xd8B\xde' len = 6
 utf-32: b'\xff\xfe\x00\x00B\xf6\x01\x00' len = 8

text: 'Aä½ å¥½ðŸ™‚B'
 utf-8 : b'A\xe4\xbd\xa0\xe5\xa5\xbd\xf0\x9f\x99\x82B' len = 12
 utf-16: b'\xff\xfeA\x00`O}Y=\xd8B\xdeB\x00' len = 14
 utf-32: b'\xff\xfe\x00\x00A\x00\x00\x00`O\x00\x00}Y\x00\x00B\xf6\x01\x00B\x00\x00\x00' len = 24
```
**Answer:** 
    "Shorter sequences, more efficient training": UTF-8 is more space-saving (especially for English/ASCII content, one byte and one character), and when including Chinese/emoji, UTF-8 is a longer encoding, but it is still much shorter than UTF-16/32 overall. 
    "Simpler and more compatible": Moreover, almost all real-world corpora are UTF-8 bytes, making it easier and more compatible to build a tokenizer directly on UTF-8 bytes.

    â€œåºåˆ—æ›´çŸ­ï¼Œè®­ç»ƒæ›´é«˜æ•ˆâ€ï¼šUTF-8 æ›´èŠ‚çœç©ºé—´ï¼ˆå°¤å…¶æ˜¯è‹±æ–‡/ASCII å†…å®¹ï¼Œä¸€å­—èŠ‚ä¸€å­—ç¬¦ï¼‰ï¼ŒåŒ…å«ä¸­æ–‡ / emoji çš„æ—¶å€™ï¼ŒUTF-8 æ˜¯å˜é•¿ç¼–ç ï¼Œä½†ä¾ç„¶æ•´ä½“æ¯” UTF-16/32 çŸ­å¾ˆå¤šï¼›
    â€œæ›´ç®€å•ã€æ›´å…¼å®¹â€ï¼šè€Œä¸”çŽ°å®žä¸–ç•Œè¯­æ–™å‡ ä¹Žéƒ½æ˜¯ UTF-8 bytesï¼Œç›´æŽ¥åœ¨ UTF-8 bytes ä¸Šå»º tokenizer æ›´ç®€å•ã€æ›´å…¼å®¹ã€‚

**ï¼ˆbï¼‰** Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into
a Unicode string. Why is this function incorrect? Provide an example of an input byte string
that yields incorrect results.
```
# Provide an example of an input byte string that yields incorrect results.
def decode_utf8_bytes_to_str_wrong(bytestring: bytes):
return "".join([bytes([b]).decode("utf-8") for b in bytestring])

>>> decode_utf8_bytes_to_str_wrong("hello".encode("utf-8"))
'hello'
```
```
def decode_utf8_bytes_to_str_wrong(bytestring: bytes) -> str:
    return "".join(bytes([b]).decode("utf-8") for b in bytestring)

def decode_utf8_bytes_to_str_ok(bytestring: bytes) -> str:
    return bytestring.decode("utf-8")

# ASCII
s_ascii = "hello"
b_ascii = s_ascii.encode("utf-8")
print("ASCII bytes:", b_ascii)
print("wrong:", decode_utf8_bytes_to_str_wrong(b_ascii))
print(" ok  :", decode_utf8_bytes_to_str_ok(b_ascii))

# non-ASCII 
s_non = "ä½ å¥½"           # æˆ–è€… "ðŸ™‚" ä¹‹ç±»
b_non = s_non.encode("utf-8")
print("\nnon-ASCII bytes:", b_non)
print("wrong:", decode_utf8_bytes_to_str_wrong(b_non))
print(" ok  :", decode_utf8_bytes_to_str_ok(b_non))

>>>
ASCII bytes: b'hello'
wrong: hello
 ok  : hello

non-ASCII bytes: b'\xe4\xbd\xa0\xe5\xa5\xbd'
Traceback (most recent call last):
  File "e:\Vscode_project\python\ceshi.py", line 18, in <module>
    print("wrong:", decode_utf8_bytes_to_str_wrong(b_non))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Vscode_project\python\ceshi.py", line 2, in decode_utf8_bytes_to_str_wrong
    return "".join(bytes([b]).decode("utf-8") for b in bytestring)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "e:\Vscode_project\python\ceshi.py", line 2, in <genexpr>
    return "".join(bytes([b]).decode("utf-8") for b in bytestring)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data
```
**Explanation:**
    For "hello", both output is the same (because ASCII is single byte in UTF-8);
    For "ä½ å¥½" / emoji characters that require a multi-byte UTF-8 sequence,"decode_utf8_bytes_to_str_ok" gets the correct string; "decode_utf8_bytes_to_str_wrong" either reports an error or becomes the result of confusion.

**Answer:**

Error examples:
    s_non = "ä½ å¥½"/"ðŸ™‚"/"Ã©"

Reason:
    This function decodes each individual byte as a full UTF-8 character, and the correct approach should be to decode the entire byte string together in UTF-8. When multi-byte characters are taken apart, errors occur.

    è¿™ä¸ªå‡½æ•°æŠŠæ¯ä¸ª å•ç‹¬çš„å­—èŠ‚ å½“æˆä¸€ä¸ªå®Œæ•´çš„ UTF-8 å­—ç¬¦åŽ»è§£ç ï¼Œè€Œæ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯æŠŠæ•´ä¸ªå­—èŠ‚ä¸²ä¸€èµ·æŒ‰ UTF-8 è§£ç ã€‚å¤šå­—èŠ‚å­—ç¬¦è¢«æ‹†å¼€ï¼Œå°±ä¼šå‡ºé”™ã€‚

**ï¼ˆcï¼‰** Give a two byte sequence that does not decode to any Unicode character(s).
```
candidates = [
    bytes([0xC3, 0x28]),
    bytes([0x80, 0x80]),
    bytes([0xFF, 0xFF]),
]

for b in candidates:
    print("testing:", b)
    try:
        print(" decoded:", b.decode("utf-8"))
    except UnicodeDecodeError as e:
        print(" UnicodeDecodeError:", e)
    print()

>>>
    testing: b'\xc3('
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xc3 in position 0: invalid continuation byte

    testing: b'\x80\x80'
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte

    testing: b'\xff\xff'
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte
```
**Answer:**

Error examples: bytes([0xC3, 0x28]).

Reason: 
*UTF-8 define:*
    The first byte of the two-byte character should be 110xxxxx ,
    The second byte must be 10xxxxxx;
    
    *UTF-8 ç¼–ç è§„åˆ™ï¼š*
    ä¸¤å­—èŠ‚å­—ç¬¦é¦–å­—èŠ‚åº”æ˜¯ 110xxxxx,
    ç¬¬äºŒä¸ªå­—èŠ‚å¿…é¡»æ˜¯ 10xxxxxxï¼›

---

## 2.4 BPE Tokenizer Training

### Example (bpe_example): BPE training example
You can find a simple code implementation of the experiment under this pathï¼š 

ä½ å¯ä»¥æ‰¾åˆ°å®žéªŒçš„ç®€æ˜“ä»£ç å®žçŽ°åœ¨è¯¥è·¯å¾„ä¸‹ï¼š
`cs336-assignments/a1-basics/exp/bpe_example.py`
ä½ å¯ä»¥æ‰¾åˆ°å®žéªŒçš„ç®€æ˜“ä»£ç å®žçŽ°åœ¨è¯¥è·¯å¾„ä¸‹ï¼š

```
# The relevant results are as followsï¼š
# ç›¸å…³è¿è¡Œç»“æžœå¦‚ä¸‹ï¼š
python a1-basics/exp/bpe_example.py

>>>
word_counts: Counter({'low': 5, 'newest': 5, 'widest': 3, 'lower': 2, 'and': 1, 'the': 1, 'vocabulary': 1, 'has': 1, 'a': 1, 'special': 1, 'token': 1, '<endoftext>.': 1})
word_tokens:
  low -> (b'l', b'o', b'w')
  lower -> (b'l', b'o', b'w', b'e', b'r')
  widest -> (b'w', b'i', b'd', b'e', b's', b't')
  newest -> (b'n', b'e', b'w', b'e', b's', b't')
  and -> (b'a', b'n', b'd')
  the -> (b't', b'h', b'e')
  vocabulary -> (b'v', b'o', b'c', b'a', b'b', b'u', b'l', b'a', b'r', b'y')
  has -> (b'h', b'a', b's')
  a -> (b'a',)
  special -> (b's', b'p', b'e', b'c', b'i', b'a', b'l')
  token -> (b't', b'o', b'k', b'e', b'n')
  <endoftext>. -> (b'<', b'e', b'n', b'd', b'o', b'f', b't', b'e', b'x', b't', b'>', b'.')

pair_counts:
 (<, e) -> 1
 (>, .) -> 1
 (a, b) -> 1
 (a, l) -> 1
 (a, n) -> 1
 (a, r) -> 1
 (a, s) -> 1
 (b, u) -> 1
 (c, a) -> 1
 (c, i) -> 1
 (d, e) -> 3
 (d, o) -> 1
 (e, c) -> 1
 (e, n) -> 2
 (e, r) -> 2
 (e, s) -> 8
 (e, w) -> 5
 (e, x) -> 1
 (f, t) -> 1
 (h, a) -> 1
 (h, e) -> 1
 (i, a) -> 1
 (i, d) -> 3
 (k, e) -> 1
 (l, a) -> 1
 (l, o) -> 7
 (n, d) -> 2
 (n, e) -> 5
 (o, c) -> 1
 (o, f) -> 1
 (o, k) -> 1
 (o, w) -> 7
 (p, e) -> 1
 (r, y) -> 1
 (s, p) -> 1
 (s, t) -> 8
 (t, >) -> 1
 (t, e) -> 1
 (t, h) -> 1
 (t, o) -> 1
 (u, l) -> 1
 (v, o) -> 1
 (w, e) -> 7
 (w, i) -> 3
 (x, t) -> 1

Best pair to merge: (b's', b't') freq: 8
  as chars: s t

newest tokens before: (b'n', b'e', b'w', b'e', b's', b't')
newest tokens after merge 's','t': (b'n', b'e', b'w', b'e', b'st')

Merges sequence:
  s t
  e st
  o w
  l ow
  w est
  n e

Final tokens per word:
  low -> ['low']
  lower -> ['low', 'e', 'r']
  widest -> ['w', 'i', 'd', 'est']
  newest -> ['ne', 'west']
  and -> ['a', 'n', 'd']
  the -> ['t', 'h', 'e']
  vocabulary -> ['v', 'o', 'c', 'a', 'b', 'u', 'l', 'a', 'r', 'y']
  has -> ['h', 'a', 's']
  a -> ['a']
  special -> ['s', 'p', 'e', 'c', 'i', 'a', 'l']
  token -> ['t', 'o', 'k', 'e', 'n']
  <endoftext>. -> ['<', 'e', 'n', 'd', 'o', 'f', 't', 'e', 'x', 't', '>', '.']

Tokenization of 'newest': ['ne', 'west']
```

---